<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>强化学习笔记 | -春弦-</title><meta name="keywords" content="强化学习"><meta name="author" content="春弦"><meta name="copyright" content="春弦"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="强化学习 强化学习的总体目标：寻找最优策略。 关键名词 智能体（Agent）  状态（State）：智能体相对于环境的状态 $s∈S$  状态空间（State space）：把所有状态放在一起，所有状态的集合（set）$S&#x3D;{s_1,s_2,…,s_n}$  动作（Action）：对于每个状态，所可能采取的行动 $a∈A(s)$  动作空间（Action space）：某状态下所可能采取的所有动作">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习笔记">
<meta property="og:url" content="https://ember0520.github.io/2025/03/19/RL_studynote/index.html">
<meta property="og:site_name" content="-春弦-">
<meta property="og:description" content="强化学习 强化学习的总体目标：寻找最优策略。 关键名词 智能体（Agent）  状态（State）：智能体相对于环境的状态 $s∈S$  状态空间（State space）：把所有状态放在一起，所有状态的集合（set）$S&#x3D;{s_1,s_2,…,s_n}$  动作（Action）：对于每个状态，所可能采取的行动 $a∈A(s)$  动作空间（Action space）：某状态下所可能采取的所有动作">
<meta property="og:locale">
<meta property="og:image" content="https://picss.sunbangyan.cn/2024/01/20/0712f90713f6c7624e847c3c6f80b006.jpeg">
<meta property="article:published_time" content="2025-03-19T07:42:08.000Z">
<meta property="article:modified_time" content="2025-03-19T07:56:47.418Z">
<meta property="article:author" content="春弦">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://picss.sunbangyan.cn/2024/01/20/0712f90713f6c7624e847c3c6f80b006.jpeg"><link rel="shortcut icon" href="/img/favicon1.png"><link rel="canonical" href="https://ember0520.github.io/2025/03/19/RL_studynote/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '强化学习笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-19 15:56:47'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/mouse.css"><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="/css/progress_bar.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="-春弦-" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/Avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li><li><a class="site-page child" href="/comments/index.html"><i class="fa-fw fas fa-solid fa-comment"></i><span> Comments</span></a></li><li><a class="site-page child" href="/bangumis/index.html"><i class="fa-fw fas fa-video"></i><span> Animation</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-solid fa-address-card"></i><span> About</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://picss.sunbangyan.cn/2024/01/20/0712f90713f6c7624e847c3c6f80b006.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">-春弦-</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li><li><a class="site-page child" href="/comments/index.html"><i class="fa-fw fas fa-solid fa-comment"></i><span> Comments</span></a></li><li><a class="site-page child" href="/bangumis/index.html"><i class="fa-fw fas fa-video"></i><span> Animation</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-solid fa-address-card"></i><span> About</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">强化学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-03-19T07:42:08.000Z" title="Created 2025-03-19 15:42:08">2025-03-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-03-19T07:56:47.418Z" title="Updated 2025-03-19 15:56:47">2025-03-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">8.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>32min</span></span><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/2025/03/19/RL_studynote/" data-flag-title="强化学习笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span class="leancloud-visitors-count"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="D:/BLOG/source/_posts/RL_studynote/p1.png" alt="学习顺序"></p>
<p>强化学习的总体目标：寻找最优策略。</p>
<h2 id="关键名词"><a href="#关键名词" class="headerlink" title="关键名词"></a>关键名词</h2><ul>
<li><p><strong>智能体（Agent）</strong></p>
</li>
<li><p><strong>状态（State）</strong>：智能体相对于环境的状态 $s∈S$</p>
</li>
<li><p><strong>状态空间（State space）</strong>：把所有状态放在一起，所有状态的集合（set）$S={s_1,s_2,…,s_n}$</p>
</li>
<li><p><strong>动作（Action）</strong>：对于每个状态，所可能采取的行动 $a∈A(s)$</p>
</li>
<li><p><strong>动作空间（Action space）</strong>：某状态下所可能采取的所有动作的集合（动作依赖于状态）$A(s)={a_1,a_2,…,a_m}$</p>
</li>
<li><p><strong>状态转移（State transition）</strong>：通过采取一个动作，智能体从一个状态进入另一个</p>
</li>
<li><p><strong>状态转移概率（State transition probability）</strong>：用条件概率描述状态转移 $P(s′∣s,a)$</p>
</li>
<li><p><strong>策略（Policy）</strong>：策略告诉智能体在某一状态下应采取什么行动</p>
<p>在数学上，策略被定义为在给定状态 $s$ 时选择行动 $a$ 的概率，即 $π(a∣s)$。</p>
<p>策略 $π$ 需要满足以下条件：对于所有状态 $s∈S$，所有可能行动 $a∈A(s)$ 的概率之和等于 1：</p>
</li>
</ul>
<script type="math/tex; mode=display">
\sum_{a∈A(s)}π(a∣s)=1</script><ul>
<li><p><strong>回报（Reward）</strong>：回报 $R(s,a)$ 是智能体在状态 $s$ 下执行动作 $a$ 后获得的标量值。它可以是正数（奖励）或负数（惩罚）。</p>
</li>
<li><p><strong>轨迹（Trajectory）和收益（Return）</strong>：轨迹是一系列状态、动作和回报的序列，形式为 $(s_0,a_0,r_0,s_1,a_1,r_1,…,s_T)$。轨迹的收益 $G$ 是沿轨迹收集的所有回报的总和：</p>
<script type="math/tex; mode=display">
G=\sum_{t=0}^{T−1}r_t</script></li>
<li><p><strong>折扣收益（Discounted Return）</strong>：在无限或长期的环境中，折扣收益用于优先考虑即时回报，而不是未来的回报。它通过折扣因子 $γ$（其中 $0≤γ≤1$）来计算：</p>
<script type="math/tex; mode=display">
G=\sum_{∑t=0}^{∞}γ_tr_t</script><p>当 $\gamma=1$ 时，它就是标准的收益。折扣因子可使得无限序列的总和是有限的，并且模型能够捕捉到即时回报比未来回报更有价值这一概念。$\gamma$越接近1越远视（注重长期回报），$\gamma$越接近0越近视（注重及时回报）。</p>
</li>
<li><p><strong>Episode</strong>：一个有限的轨迹，通常在特定的终止状态处结束。</p>
</li>
<li><p><strong>马尔科夫决策过程（Markov decision process, MDP)</strong>：MDP 的目标是找到一个策略 $π$，使得从起始状态开始的预期折扣收益最大化。具有无记忆性（和之前的状态无关）。</p>
</li>
</ul>
<script type="math/tex; mode=display">
π^∗=\mathrm{argmax_{\pi}}E_π[G∣s0]</script><p>其中 $E_π[G∣s0]$ 表示在策略 $π$ 下，从初始状态 $s_0$ 开始的预期折扣收益。</p>
<p>MDP 由以下组件定义：</p>
<ul>
<li>状态空间 $S$</li>
<li>动作空间 $A(s)$</li>
<li>状态转移概率 $P(s′∣s,a)$</li>
<li>回报函数 $R(s,a)$</li>
<li>折扣因子 $γ$</li>
</ul>
<hr>
<h2 id="贝尔曼公式"><a href="#贝尔曼公式" class="headerlink" title="贝尔曼公式"></a>贝尔曼公式</h2><h3 id="状态值函数（State-Value-Function）"><a href="#状态值函数（State-Value-Function）" class="headerlink" title="状态值函数（State Value Function）"></a>状态值函数（State Value Function）</h3><p><strong>定义</strong>：<br>在策略$\pi$下从状态$s$出发的<strong>预期折扣收益</strong>，其定义为：</p>
<script type="math/tex; mode=display">
V^{\pi}(s) = \mathbb{E}_{\pi} \left[ G \mid s_0 = s \right] = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \bigg| s_0 = s \right]</script><p>其中：</p>
<ul>
<li>$\mathbb{E}_{\pi}[\cdot]$表示在策略$\pi $下的期望，覆盖动作选择和状态转移的随机性。</li>
<li>$G = \sum_{t=0}^{\infty} \gamma^t r_t$是折扣收益，$\gamma \in [0,1)$是折扣因子，确保无穷级数收敛。</li>
<li>$r<em>t = R(s_t, a_t)$是时刻$t$的即时回报，$a_t \sim \pi(\cdot \mid s_t)$，状态转移由$s</em>{t+1} \sim P(\cdot \mid s_t, a_t)$决定。</li>
</ul>
<h3 id="贝尔曼方程（Bellman-Equation）"><a href="#贝尔曼方程（Bellman-Equation）" class="headerlink" title="贝尔曼方程（Bellman Equation）"></a>贝尔曼方程（Bellman Equation）</h3><p>状态值函数满足递归关系：</p>
<script type="math/tex; mode=display">
V^{\pi}(s) = \sum_{a \in A} \pi(a \mid s) \left[ R(s,a) + \gamma \sum_{s' \in S} P(s' \mid s,a) V^{\pi}(s') \right], \quad \forall s \in S</script><p><strong>含义</strong>：<br>当前状态的值等于即时回报的期望，加上未来状态的期望折扣值。</p>
<p><strong>矩阵形式</strong>：  </p>
<script type="math/tex; mode=display">
\mathbf{V}^{\pi} = \mathbf{R}^{\pi} + \gamma \mathbf{P}^{\pi} \mathbf{V}^{\pi}</script><p>其闭式解为：</p>
<script type="math/tex; mode=display">
\mathbf{V}^{\pi} = (I - \gamma \mathbf{P}^{\pi})^{-1} \mathbf{R}^{\pi}</script><p>当 $\gamma &lt; 1$ 时，矩阵$(I - \gamma \mathbf{P}^{\pi})$可逆。</p>
<h3 id="动作值函数（Action-Value-Function）"><a href="#动作值函数（Action-Value-Function）" class="headerlink" title="动作值函数（Action Value Function）"></a>动作值函数（Action Value Function）</h3><p><strong>定义</strong>：<br>动作值函数$Q^{\pi}(s,a)$表示从状态$s$执行动作$a$后，遵循策略$ \pi $的预期折扣收益：</p>
<script type="math/tex; mode=display">
Q^{\pi}(s,a) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \bigg| s_0 = s, a_0 = a \right]</script><p><strong>与状态值函数的关系</strong>：<br>状态值函数可表示为动作值函数的期望：</p>
<script type="math/tex; mode=display">
V^{\pi}(s) = \sum_{a \in A} \pi(a \mid s) Q^{\pi}(s,a)</script><p><strong>动作值的贝尔曼方程</strong>：<br>动作值函数同样满足递归关系：</p>
<script type="math/tex; mode=display">
Q^{\pi}(s,a) = R(s,a) + \gamma \sum_{s' \in S} P(s' \mid s,a) V^{\pi}(s')</script><hr>
<h2 id="贝尔曼最优公式"><a href="#贝尔曼最优公式" class="headerlink" title="贝尔曼最优公式"></a>贝尔曼最优公式</h2><h3 id="最优策略与最优值函数"><a href="#最优策略与最优值函数" class="headerlink" title="最优策略与最优值函数"></a>最优策略与最优值函数</h3><p><strong>最优策略定义</strong>：<br>策略$\pi^*$是最优的，当且仅当对任意状态$s$，其满足：</p>
<script type="math/tex; mode=display">
\pi^*(a \mid s) = \begin{cases} 
1, & a = \arg\max_{a'} Q^{\pi^*}(s,a') \\
0, & \text{其他}
\end{cases}</script><p><strong>最优状态值函数</strong>：<br>最优值函数$V^*(s)$是所有策略中最大的状态值：</p>
<script type="math/tex; mode=display">
V^*(s) = \max_{\pi} V^{\pi}(s)</script><p><strong>贝尔曼最优方程</strong>：<br>最优值函数满足：</p>
<script type="math/tex; mode=display">
V^*(s) = \max_{a \in A} \left[ R(s,a) + \gamma \sum_{s' \in S} P(s' \mid s,a) V^*(s') \right], \quad \forall s \in S</script><p>对应动作值函数的最优方程为：</p>
<script type="math/tex; mode=display">
Q^*(s,a) = R(s,a) + \gamma \sum_{s' \in S} P(s' \mid s,a) \max_{a'} Q^*(s',a')</script><p><strong>存在性与唯一性</strong></p>
<ol>
<li><strong>存在性</strong>：在有限马尔可夫决策过程MDP中，若$\gamma &lt; 1$，则存在唯一的最优值函数$V^<em>$和$Q^</em>$，以及至少一个确定性最优策略 $\pi^*$。</li>
<li><strong>唯一性</strong>：存在唯一最优解（最优策略不一定唯一）。放缩映射证明。</li>
<li><strong>收敛性</strong>：通过值迭代或策略迭代算法，可逐步逼近$V^<em>$ 和$\pi^</em>$（指数级收敛）。</li>
</ol>
<hr>
<h2 id="值迭代（Value-Iteration）"><a href="#值迭代（Value-Iteration）" class="headerlink" title="值迭代（Value Iteration）"></a>值迭代（Value Iteration）</h2><p><strong>基本思想</strong></p>
<p>通过<strong>直接迭代贝尔曼最优方程</strong>求解最优值函数，最终从最优值函数中提取最优策略。动态规划思想，通过同步备份（synchronous backup）更新所有状态的值。</p>
<p><strong>算法步骤</strong></p>
<script type="math/tex; mode=display">
V_{k}(s)\to Q_{k}(s,a) \to \pi_{k+1}(a|s) \to V_{k+1}(s) \to \max_aQ_{k}(s,a)</script><ol>
<li><p><strong>初始化</strong>：<br>对所有状态$s \in S$，设置初始值$V_0(s) = 0$（或其他任意值）</p>
</li>
<li><p><strong>迭代更新</strong>：<br>重复以下更新直至收敛：</p>
<script type="math/tex; mode=display">
V_{k+1}(s) = \max_{a \in A} \left[ R(s,a) + \gamma \sum_{s' \in S} P(s' \mid s,a) V_k(s') \right], \quad \forall s \in S</script><p>更新方式为<strong>同步备份</strong>（先计算所有新值，再整体替换旧值）</p>
</li>
<li><p><strong>终止条件</strong>：<br>当$\max<em>{s \in S} |V</em>{k+1}(s) - V_k(s)| &lt; \varepsilon$（预设阈值）时停止</p>
</li>
<li><p><strong>策略提取</strong>：<br>最终通过最优值函数$V^*$得到确定性策略：</p>
<script type="math/tex; mode=display">
\pi^*(s) = \arg\max_{a \in A} \left[ R(s,a) + \gamma \sum_{s' \in S} P(s' \mid s,a) V^*(s') \right]</script></li>
</ol>
<p><strong>特性分析</strong></p>
<ol>
<li><p><strong>收敛性</strong>：  </p>
<ul>
<li>当$\gamma &lt; 1$时，值迭代以指数速度收敛到唯一最优解</li>
<li>迭代次数与状态数无关，仅依赖$\gamma$和$\varepsilon$</li>
</ul>
</li>
<li><p><strong>时间复杂度</strong>：<br>每轮迭代复杂度为$O(|S|^2|A|)$，适用于状态空间较小的问题</p>
</li>
<li><p><strong>与贝尔曼方程关系</strong>：<br>值迭代本质是不断应用贝尔曼最优算子的不动点迭代</p>
</li>
</ol>
<hr>
<h2 id="策略迭代（Policy-Iteration）"><a href="#策略迭代（Policy-Iteration）" class="headerlink" title="策略迭代（Policy Iteration）"></a>策略迭代（Policy Iteration）</h2><p><strong>基本思想</strong></p>
<p>通过<strong>交替进行策略评估（Policy Evaluation）和策略改进（Policy Improvement）</strong>来优化策略，直到收敛到最优策略。</p>
<p><strong>算法步骤</strong></p>
<ol>
<li><p><strong>初始化</strong>：<br>随机选择一个初始策略$\pi_0$</p>
</li>
<li><p><strong>策略迭代循环</strong>：<br><strong>Repeat</strong>:  </p>
<ul>
<li><p><strong>(1) 策略评估</strong>：<br>计算当前策略$\pi_k$的值函数$V^{\pi_k}$<br>通过求解贝尔曼方程：</p>
<script type="math/tex; mode=display">
V^{\pi_k}(s) = \sum_{a} \pi_k(a|s) \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^{\pi_k}(s') \right]</script><p>可通过<strong>迭代法</strong>（重复应用上式直至收敛）或<strong>直接求解线性方程组</strong>获得精确解</p>
</li>
<li><p><strong>(2) 策略改进</strong>：<br>对每个状态$s$，选择使动作值最大的动作：</p>
<script type="math/tex; mode=display">
\pi_{k+1}(s) = \arg\max_{a} \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^{\pi_k}(s') \right]</script></li>
</ul>
<p><strong>Until</strong> $\pi_{k+1} = \pi_k$（策略稳定，实际上是无限逼近）</p>
</li>
</ol>
<p><strong>特性分析</strong></p>
<ol>
<li><p><strong>收敛速度</strong>：  </p>
<ul>
<li>通常比值迭代更快（尤其当策略空间较小时）</li>
<li>策略改进阶段保证每次迭代策略至少不劣化</li>
</ul>
</li>
<li><p><strong>计算复杂度</strong>：  </p>
<ul>
<li>策略评估阶段需要$O(|S|^3)$（直接求解）或$O(m|S|^2)$（迭代m次）</li>
<li>适用于中等规模状态空间问题</li>
</ul>
</li>
</ol>
<hr>
<h2 id="截断策略迭代（Truncated-Policy-Iteration）"><a href="#截断策略迭代（Truncated-Policy-Iteration）" class="headerlink" title="截断策略迭代（Truncated Policy Iteration）"></a>截断策略迭代（Truncated Policy Iteration）</h2><p><strong>基本思想</strong></p>
<p>在标准策略迭代的基础上，<strong>放宽策略评估的精度要求</strong>。通过<strong>限制策略评估阶段的迭代次数</strong>（如固定次数$k$次），提前截断对当前策略的值函数计算，以降低每次迭代的计算量，同时仍能保证策略逐步优化。</p>
<p><strong>算法步骤</strong></p>
<ol>
<li><p><strong>初始化</strong>：  </p>
<ul>
<li>随机初始化策略$\pi_0$  </li>
<li>设置策略评估阶段的迭代次数上限$k$（例如$k=3$）</li>
</ul>
</li>
<li><p><strong>策略迭代循环</strong>：<br><strong>Repeat</strong>:  </p>
<ul>
<li><p><strong>(1) 截断策略评估</strong>：<br>对当前策略$\pi_i$，执行以下步骤（从初始值函数$V_0$开始）:  </p>
<ul>
<li><p><strong>For</strong> $t=0$ to $k-1$ <strong>do</strong>:<br>更新值函数：  </p>
<script type="math/tex; mode=display">
V_{t+1}(s) = \sum_{a} \pi_i(a|s) \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V_t(s') \right]</script></li>
<li><p>最终得到近似值函数$V_k \approx V^{\pi_i}$  </p>
</li>
</ul>
</li>
<li><p><strong>(2) 策略改进</strong>：<br>基于近似值函数$V_k$，贪婪更新策略：  </p>
<script type="math/tex; mode=display">
\pi_{i+1}(s) = \arg\max_{a} \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V_k(s') \right]</script></li>
</ul>
<p><strong>Until</strong> $\pi_{i+1} = \pi_i$（策略稳定）</p>
</li>
</ol>
<p><strong>特性分析</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="D:/BLOG/source/_posts/RL_studynote/p2.png" alt="收敛速度"></p>
<ol>
<li><strong>收敛性</strong>：  <ul>
<li>即使策略评估未完全收敛，只要策略改进阶段能提升策略，算法仍能收敛到最优策略</li>
<li>收敛速度可能慢于标准策略迭代，但快于值迭代</li>
</ul>
</li>
<li><strong>精度-效率权衡</strong>：  <ul>
<li>增大$k$：策略评估更精确，策略改进更有效，但单次迭代时间增加  </li>
<li>减小$k$：单次迭代更快，但可能需要更多轮次策略迭代</li>
</ul>
</li>
</ol>
<hr>
<h2 id="蒙特卡洛方法（Monte-Carlo）"><a href="#蒙特卡洛方法（Monte-Carlo）" class="headerlink" title="蒙特卡洛方法（Monte Carlo）"></a>蒙特卡洛方法（Monte Carlo）</h2><p><strong>基本思想</strong></p>
<p>蒙特卡洛方法是一种<strong>model-free</strong>的强化学习方法，不依赖环境模型（即无需已知转移概率 $P(s’ \mid s,a)$ 和奖励函数 $R(s,a)$）（隐式，未知给智能体）。它通过<strong>直接与环境交互生成样本轨迹（episode）</strong>，利用样本的回报（return）均值来估计状态值函数或动作值函数。核心是用<strong>经验平均</strong>代替<strong>期望计算</strong>，公式化表示为：</p>
<script type="math/tex; mode=display">
V^{\pi}(s) \approx \frac{1}{N(s)} \sum_{i=1}^{N(s)} G_i(s)</script><p>其中 $G_i(s)$ 是状态 $s$ 在第 $i$ 次轨迹中的累积回报，$N(s)$ 是 $s$ 被访问的次数。</p>
<p><strong>核心步骤</strong></p>
<ul>
<li>策略评估（Policy Evaluation）</li>
</ul>
<p>通过采样轨迹估计当前策略 $\pi$ 的值函数：</p>
<p><strong>首次访问型（First-visit MC）</strong>：仅统计每个状态 $s$ 在一条轨迹中第一次出现时的回报  </p>
<p><strong>每次访问型（Every-visit MC）</strong>：统计每条轨迹中每次访问状态 $s$ 的回报  </p>
<p><strong>增量式更新公式</strong>（优化存储效率）：</p>
<script type="math/tex; mode=display">
V(s) \leftarrow V(s) + \alpha \left[ G(s) - V(s) \right]</script><p>其中 $\alpha$ 为学习率，$G(s)$ 是本次轨迹中 $s$ 的回报。</p>
<ul>
<li>策略控制（Policy Control）</li>
</ul>
<p>通过优化动作值函数 $Q(s,a)$ 改进策略，常用两种方法：</p>
<p>a. 同轨策略（On-policy）</p>
<p>使用<strong>与采样相同的策略</strong>进行改进  </p>
<p>常用 $\varepsilon$-<strong>贪婪策略</strong>平衡探索与利用：</p>
<script type="math/tex; mode=display">
\pi(a \mid s) = \begin{cases} 
1- \frac{|A(s)|-1}{|A(s)|}\varepsilon, & a = \arg\max_{a'} Q(s,a') \\
\frac{\varepsilon}{|A(s)|}, & \text{其他动作}
\end{cases}</script><p>b. 离轨策略（Off-policy）</p>
<p>使用<strong>行为策略（behavior policy）</strong>生成轨迹，但优化<strong>目标策略（target policy）</strong>  </p>
<p>通过<strong>重要性采样（Importance Sampling）</strong>修正回报：</p>
<script type="math/tex; mode=display">
V^{\pi}(s) = \mathbb{E}_{\pi_b} \left[ \frac{\pi(a \mid s)}{\pi_b(a \mid s)} G(s) \right]</script><p>算法流程（首次访问型蒙特卡洛控制）</p>
<ol>
<li><p><strong>初始化</strong>：  </p>
<ul>
<li>随机初始化动作值函数 $Q(s,a)$  </li>
<li>定义 $\varepsilon$-贪婪策略 $\pi$  </li>
</ul>
</li>
<li><p><strong>生成轨迹</strong>：<br>使用当前策略 $\pi$ 与环境交互，生成完整轨迹  </p>
<script type="math/tex; mode=display">s_0, a_0, r_1, s_1, a_1, r_2, \dots, s_T</script></li>
<li><p><strong>更新动作值函数</strong>：<br>对轨迹中每个状态-动作对 $(s_t, a_t)$：  </p>
<ul>
<li>计算累积回报 $G<em>t = \sum</em>{k=t}^{T} \gamma^{k-t} r_{k+1}$  </li>
<li>更新 $Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha (G_t - Q(s_t, a_t))$  </li>
</ul>
</li>
<li><p><strong>策略改进</strong>：<br>根据更新后的 $Q$ 函数，调整 $\varepsilon$-贪婪策略：</p>
<script type="math/tex; mode=display">
\pi(s) = \begin{cases} 
\arg\max_{a} Q(s,a), & \text{以概率 } 1-\varepsilon \\
\text{随机动作}, & \text{以概率 } \varepsilon
\end{cases}</script></li>
<li><p><strong>重复</strong>步骤2-4，直到 $Q$ 函数收敛。</p>
</li>
</ol>
<hr>
<h2 id="随机近似（Stochastic-Approximation）"><a href="#随机近似（Stochastic-Approximation）" class="headerlink" title="随机近似（Stochastic Approximation）"></a>随机近似（Stochastic Approximation）</h2><p>基本思想</p>
<p>随机近似是一类通过<strong>带噪声的观测数据迭代逼近目标值</strong>的数学方法，核心是用随机样本逐步修正估计值。其广泛应用于求解无法直接计算期望的优化问题或方程根，尤其在强化学习、信号处理等领域。</p>
<p><strong>Robbins-Monro 算法</strong></p>
<p><strong>问题设定</strong>：寻找方程 $g(\theta) = 0$ 的根 $\theta^*$，其中 $g(\theta)$ 的精确值未知，但可通过噪声观测 $Y(\theta) = g(\theta) + \epsilon$ 获取信息。</p>
<p><strong>迭代公式</strong>：</p>
<script type="math/tex; mode=display">
\theta_{k+1} = \theta_k - \alpha_k Y(\theta_k)</script><p>其中：</p>
<ul>
<li><p>$\alpha_k$ 为步长序列，需满足：</p>
<script type="math/tex; mode=display">
\sum_{k=1}^\infty \alpha_k = \infty \quad \text{且} \quad \sum_{k=1}^\infty \alpha_k^2 < \infty</script><p>例如 $\alpha_k = 1/k$。</p>
</li>
<li><p>$\epsilon$ 是零均值噪声（$\mathbb{E}[\epsilon] = 0$）。</p>
</li>
</ul>
<p>收敛性保证</p>
<ul>
<li><p><strong>条件</strong>：  </p>
<ol>
<li>函数 $g(\theta)$ 单调递增且满足增长条件（如 $\theta g(\theta) &gt; 0$ 当 $|\theta|$ 足够大）  </li>
<li>噪声方差有界：$\mathbb{E}[\epsilon^2] \leq C$  </li>
<li>步长序列满足Robbins-Monro条件  </li>
</ol>
</li>
<li><p><strong>结论</strong>：  </p>
<script type="math/tex; mode=display">
\lim_{k \to \infty} \theta_k \overset{a.s.}{\longrightarrow} \theta^*</script></li>
</ul>
<h2 id="随机梯度下降（Stochastic-Gradient-Descent-SGD）"><a href="#随机梯度下降（Stochastic-Gradient-Descent-SGD）" class="headerlink" title="随机梯度下降（Stochastic Gradient Descent, SGD）"></a>随机梯度下降（Stochastic Gradient Descent, SGD）</h2><p><strong>基本思想</strong></p>
<p>SGD是<strong>随机近似在优化问题中的特例</strong>，用于最小化经验风险函数：</p>
<script type="math/tex; mode=display">
J(\theta) = \mathbb{E}_{(x,y)\sim \mathcal{D}} [L(f_\theta(x), y)]</script><p>通过每次迭代随机采样一个或少量样本计算梯度估计，更新参数。</p>
<p>算法步骤</p>
<ol>
<li><p><strong>初始化参数</strong>：$\theta_0 \in \mathbb{R}^d$  </p>
</li>
<li><p><strong>迭代更新</strong>：<br>对于 $k=1,2,\dots$：  </p>
<ul>
<li><p>随机采样小批量样本 $\mathcal{B}<em>k = {(x_i,y_i)}</em>{i=1}^m$  </p>
</li>
<li><p>计算随机梯度估计：</p>
<script type="math/tex; mode=display">
\hat{g}_k = \frac{1}{m} \sum_{(x_i,y_i)\in \mathcal{B}_k} \nabla_\theta L(f_\theta(x_i), y_i)</script></li>
<li><p>更新参数：</p>
<script type="math/tex; mode=display">
\theta_{k+1} = \theta_k - \alpha_k \hat{g}_k</script></li>
</ul>
</li>
</ol>
<p>关键特性</p>
<ol>
<li><strong>计算效率</strong>：  <ul>
<li>每轮迭代复杂度为 $O(md)$，远低于批量梯度下降的 $O(Nd)$（$N$为总样本数）  </li>
</ul>
</li>
<li><strong>隐式正则化</strong>：  <ul>
<li>随机噪声使SGD倾向于找到平坦的极小值，提升泛化能力  </li>
</ul>
</li>
<li><strong>收敛性</strong>：  <ul>
<li>在凸问题中，SGD以 $O(1/\sqrt{k})$ 速率收敛  </li>
<li>非凸问题中收敛到局部极小值或鞍点  </li>
</ul>
</li>
</ol>
<p><strong>与批量梯度下降对比</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>特性</th>
<th>SGD</th>
<th>批量梯度下降（BGD）</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>每轮计算量</strong></td>
<td>低（小批量）</td>
<td>高（全数据集）</td>
</tr>
<tr>
<td><strong>收敛路径</strong></td>
<td>震荡较大</td>
<td>平滑</td>
</tr>
<tr>
<td><strong>局部极小值逃脱</strong></td>
<td>更强（噪声帮助逃离鞍点）</td>
<td>较弱</td>
</tr>
<tr>
<td><strong>在线学习</strong></td>
<td>支持</td>
<td>不支持</td>
</tr>
</tbody>
</table>
</div>
<p><strong>实际应用示例：线性回归</strong></p>
<p><strong>目标函数</strong>：</p>
<script type="math/tex; mode=display">
J(w) = \frac{1}{2N} \sum_{i=1}^N (w^T x_i - y_i)^2</script><p><strong>SGD更新步骤</strong>：</p>
<ol>
<li><p>随机采样样本 $(x_i, y_i)$  </p>
</li>
<li><p>计算梯度：</p>
<script type="math/tex; mode=display">
\nabla_w L = (w^T x_i - y_i) x_i</script></li>
<li><p>更新权重：</p>
<script type="math/tex; mode=display">
w_{k+1} = w_k - \alpha (w_k^T x_i - y_i) x_i</script></li>
</ol>
<p><strong>P.S.:</strong></p>
<ol>
<li><strong>SA与SGD的统一性</strong>：<br>SGD可视为求解 $\nabla J(\theta) = 0$ 的随机近似过程。</li>
<li><strong>步长设计</strong>：  <ul>
<li>恒定步长：快速收敛但可能震荡  </li>
<li>递减步长：保证收敛但速度变慢  </li>
<li>自适应步长（如Adam）：平衡速度与稳定性  </li>
</ul>
</li>
<li><strong>噪声的双重作用</strong>：  <ul>
<li>负面：增加方差，延缓收敛  </li>
<li>正面：帮助逃离局部极小，提升泛化  </li>
</ul>
</li>
</ol>
<hr>
<h2 id="时序差分学习（Temporal-Difference-Learning）"><a href="#时序差分学习（Temporal-Difference-Learning）" class="headerlink" title="时序差分学习（Temporal Difference Learning）"></a>时序差分学习（Temporal Difference Learning）</h2><h3 id="TD-Learning-of-State-Values-TD-0"><a href="#TD-Learning-of-State-Values-TD-0" class="headerlink" title="TD Learning of State Values (TD(0))"></a>TD Learning of State Values (TD(0))</h3><p><strong>基本思想</strong>  </p>
<p>通过<strong>自举（bootstrapping）</strong>结合当前估计值与即时奖励，在线更新状态值函数，无需等待完整轨迹结束。</p>
<p><strong>算法步骤</strong>  </p>
<ol>
<li><p>初始化值函数 $V(s)$  </p>
</li>
<li><p>在每个时间步 $t$：  </p>
<ul>
<li><p>观察当前状态 $s<em>t$，执行动作 $a_t$，获得奖励 $r</em>{t+1}$，转移到状态 $s_{t+1}$  </p>
</li>
<li><p>计算 <strong>TD误差</strong>：  </p>
<script type="math/tex; mode=display">
\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)</script></li>
<li><p>更新值函数：  </p>
<script type="math/tex; mode=display">
V(s_t) \leftarrow V(s_t) + \alpha \delta_t</script><p>（$\alpha$ 为学习率）</p>
</li>
</ul>
</li>
</ol>
<p><strong>特性</strong>  </p>
<ul>
<li><strong>在线更新</strong>：单步即可更新，无需等待轨迹结束  </li>
<li><strong>方差较低</strong>：相比蒙特卡洛方法，TD误差的方差更小  </li>
<li><strong>收敛性</strong>：在策略下，$V(s)$ 以概率1收敛到真实值函数 $V^\pi(s)$  </li>
</ul>
<hr>
<h3 id="TD-Learning-of-Action-Values-Sarsa"><a href="#TD-Learning-of-Action-Values-Sarsa" class="headerlink" title="TD Learning of Action Values: Sarsa"></a>TD Learning of Action Values: Sarsa</h3><p><strong>基本思想</strong>  </p>
<p><strong>同轨策略（on-policy）</strong>更新动作值函数 $Q(s,a)$，使用当前策略选择下一个动作 $a’$，更新公式为：</p>
<script type="math/tex; mode=display">
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t) \right]</script><p><strong>算法步骤</strong>  </p>
<ol>
<li>初始化 $Q(s,a)$，定义 $\varepsilon$-贪婪策略  </li>
<li>生成轨迹：  <ul>
<li>在状态 $s_t$ 选择动作 $a_t$（按 $\varepsilon$-贪婪策略）  </li>
<li>执行 $a<em>t$，获得 $r</em>{t+1}$ 和 $s_{t+1}$  </li>
<li>在 $s<em>{t+1}$ 选择动作 $a</em>{t+1}$（同样按 $\varepsilon$-贪婪策略）  </li>
</ul>
</li>
<li>更新 $Q(s_t,a_t)$  </li>
<li>重复直到收敛  </li>
</ol>
<p><strong>特性</strong>  </p>
<ul>
<li><strong>策略依赖</strong>：更新依赖于当前策略的后续动作选择  </li>
<li><strong>安全探索</strong>：通过 $\varepsilon$-贪婪平衡探索与利用  </li>
<li><strong>收敛性</strong>：需满足无限访问条件，且在策略下收敛到最优 $Q^\pi$  </li>
</ul>
<hr>
<h3 id="TD-Learning-of-Action-Values-Expected-Sarsa"><a href="#TD-Learning-of-Action-Values-Expected-Sarsa" class="headerlink" title="TD Learning of Action Values: Expected Sarsa"></a>TD Learning of Action Values: Expected Sarsa</h3><p><strong>基本思想</strong>  </p>
<p>改进Sarsa，使用<strong>期望值</strong>替代下一个动作的采样值，减少方差：</p>
<script type="math/tex; mode=display">
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \left[ r_{t+1} + \gamma \mathbb{E}_{a'} Q(s_{t+1},a') - Q(s_t,a_t) \right]</script><p>其中期望值计算为：</p>
<script type="math/tex; mode=display">
\mathbb{E}_{a'} Q(s_{t+1},a') = \sum_{a'} \pi(a'|s_{t+1}) Q(s_{t+1},a')</script><p><strong>算法步骤</strong>  </p>
<p>与Sarsa类似，但更新时计算所有可能动作的期望值而非采样单个动作。</p>
<p><strong>特性</strong>  </p>
<ul>
<li><strong>方差更低</strong>：相比Sarsa，消除了下一个动作的随机性  </li>
<li><strong>灵活性</strong>：可离线策略使用（例如目标策略与行为策略不同）  </li>
<li><strong>计算开销</strong>：需遍历所有动作计算期望，适合动作空间较小的问题  </li>
</ul>
<hr>
<h3 id="TD-Learning-of-Action-Values-n-step-Sarsa"><a href="#TD-Learning-of-Action-Values-n-step-Sarsa" class="headerlink" title="TD Learning of Action Values: n-step Sarsa"></a>TD Learning of Action Values: n-step Sarsa</h3><p><strong>基本思想</strong>  </p>
<p>结合蒙特卡洛的多步回报与TD的自举，平衡偏差与方差。定义 <strong>n步回报</strong>：</p>
<script type="math/tex; mode=display">
G_{t:t+n} = r_{t+1} + \gamma r_{t+2} + \dots + \gamma^{n-1} r_{t+n} + \gamma^n Q(s_{t+n},a_{t+n})</script><p>更新公式：</p>
<script type="math/tex; mode=display">
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \left[ G_{t:t+n} - Q(s_t,a_t) \right]</script><p><strong>算法步骤</strong>  </p>
<ol>
<li>初始化 $Q(s,a)$，生成轨迹  </li>
<li>对每个状态-动作对 $(s_t,a_t)$，累积后续 $n$ 步的奖励  </li>
<li>使用 $n$ 步回报更新 $Q(s_t,a_t)$  </li>
</ol>
<p><strong>特性</strong>  </p>
<ul>
<li><strong>n=1</strong>：退化为Sarsa  </li>
<li><strong>n→∞</strong>：退化为蒙特卡洛方法  </li>
<li><strong>折中效果</strong>：n步平衡即时奖励与长期回报的估计  </li>
</ul>
<hr>
<h3 id="TD-Learning-of-Optimal-Action-Values-Q-learning"><a href="#TD-Learning-of-Optimal-Action-Values-Q-learning" class="headerlink" title="TD Learning of Optimal Action Values: Q-learning"></a>TD Learning of Optimal Action Values: Q-learning</h3><p><strong>基本思想</strong>  </p>
<p><strong>离轨策略（off-policy）</strong>直接学习最优动作值函数 $Q^*$，更新公式为：</p>
<script type="math/tex; mode=display">
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1},a') - Q(s_t,a_t) \right]</script><p><strong>算法步骤</strong>  </p>
<ol>
<li>初始化 $Q(s,a)$，定义行为策略（如 $\varepsilon$-贪婪）  </li>
<li>生成轨迹：  <ul>
<li>在状态 $s_t$ 选择动作 $a_t$（按行为策略）  </li>
<li>执行 $a<em>t$，获得 $r</em>{t+1}$ 和 $s_{t+1}$  </li>
</ul>
</li>
<li>更新 $Q(s<em>t,a_t)$ 时使用 $\max</em>{a’} Q(s_{t+1},a’)$（目标策略为贪婪策略）  </li>
<li>重复直到收敛  </li>
</ol>
<p><strong>特性</strong>  </p>
<ul>
<li><strong>离轨策略</strong>：行为策略（探索）与目标策略（利用）分离  </li>
<li><strong>直接优化</strong>：通过最大化操作直接逼近最优策略  </li>
<li><strong>收敛性</strong>：在有限MDP中，以概率1收敛到 $Q^*$  </li>
</ul>
<hr>
<h3 id="对比总结"><a href="#对比总结" class="headerlink" title="对比总结"></a>对比总结</h3><div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>更新公式</th>
<th>策略类型</th>
<th>关键特性</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TD(0)</strong></td>
<td>$V(s) \leftarrow V(s) + \alpha [r + \gamma V(s’) - V(s)]$</td>
<td>同轨策略</td>
<td>低方差，在线更新</td>
</tr>
<tr>
<td><strong>Sarsa</strong></td>
<td>$Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma Q(s’,a’) - Q(s,a)]$</td>
<td>同轨策略</td>
<td>依赖当前策略，安全探索</td>
</tr>
<tr>
<td><strong>Expected Sarsa</strong></td>
<td>$Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \mathbb{E}_{a’} Q(s’,a’) - Q(s,a)]$</td>
<td>可离轨策略</td>
<td>低方差，计算期望</td>
</tr>
<tr>
<td><strong>n-step Sarsa</strong></td>
<td>$Q(s,a) \leftarrow Q(s,a) + \alpha [G_{t:t+n} - Q(s,a)]$</td>
<td>同轨策略</td>
<td>平衡TD与MC，多步回报</td>
</tr>
<tr>
<td><strong>Q-learning</strong></td>
<td>$Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a’} Q(s’,a’) - Q(s,a)]$</td>
<td>离轨策略</td>
<td>直接优化，最大化操作</td>
</tr>
</tbody>
</table>
</div>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="D:/BLOG/source/_posts/RL_studynote/p3.png" alt="算法对比1"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="D:/BLOG/source/_posts/RL_studynote/p4.png" alt="算法对比2"></p>
<hr>
<h2 id="值函数近似（Value-Function-Approximation）"><a href="#值函数近似（Value-Function-Approximation）" class="headerlink" title="值函数近似（Value Function Approximation）"></a>值函数近似（Value Function Approximation）</h2><h3 id="算法框架：状态值函数估计"><a href="#算法框架：状态值函数估计" class="headerlink" title="算法框架：状态值函数估计"></a>算法框架：状态值函数估计</h3><p>当状态空间巨大或连续时，无法用表格存储每个状态值。使用参数化函数 $V(s; \mathbf{w}) \approx V^\pi(s)$ 近似真实值函数，其中 $\mathbf{w}$ 为可调参数。</p>
<p><strong>算法步骤</strong>  </p>
<ol>
<li><p><strong>初始化参数</strong>：随机初始化权重 $\mathbf{w}$  </p>
</li>
<li><p><strong>交互采样</strong>：通过策略 $\pi$ 生成轨迹，收集经验 $(s<em>t, r</em>{t+1}, s_{t+1})$  </p>
</li>
<li><p><strong>计算TD误差</strong>：  </p>
<script type="math/tex; mode=display">
\delta_t = r_{t+1} + \gamma V(s_{t+1}; \mathbf{w}) - V(s_t; \mathbf{w})</script></li>
<li><p><strong>更新参数</strong>：沿减小TD误差的方向调整 $\mathbf{w}$：  </p>
<script type="math/tex; mode=display">
\mathbf{w} \leftarrow \mathbf{w} + \alpha \delta_t \nabla_{\mathbf{w}} V(s_t; \mathbf{w})</script><p>（$\alpha$ 为学习率）</p>
</li>
</ol>
<p><strong>目标函数</strong>  </p>
<p>最小化均方误差（Mean Squared Error, MSE）：  </p>
<script type="math/tex; mode=display">
J(\mathbf{w}) = \mathbb{E}_{(s) \sim \pi} \left[ \left( V^\pi(s) - V(s; \mathbf{w}) \right)^2 \right]</script><p>实际中通过采样近似：  </p>
<script type="math/tex; mode=display">
J(\mathbf{w}) \approx \frac{1}{N} \sum_{i=1}^N \left( V^\pi(s_i) - V(s_i; \mathbf{w}) \right)^2</script><p><strong>优化算法</strong>  </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
<th>公式</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>随机梯度下降</strong></td>
<td>逐样本更新，降低计算量</td>
<td>$\mathbf{w} \leftarrow \mathbf{w} - \alpha \nabla_{\mathbf{w}} J(\mathbf{w})$</td>
</tr>
<tr>
<td><strong>批量梯度下降</strong></td>
<td>全数据集计算梯度，收敛稳定但效率低</td>
<td>$\mathbf{w} \leftarrow \mathbf{w} - \alpha \sum<em>{i=1}^N \nabla</em>{\mathbf{w}} J_i(\mathbf{w})$</td>
</tr>
<tr>
<td><strong>小批量梯度下降</strong></td>
<td>折中方案，常用在深度学习</td>
<td>$\mathbf{w} \leftarrow \mathbf{w} - \frac{\alpha}{m} \sum<em>{i=1}^m \nabla</em>{\mathbf{w}} J_i(\mathbf{w})$</td>
</tr>
<tr>
<td><strong>最小二乘法</strong></td>
<td>仅适用于线性模型，解析解高效</td>
<td>$\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$</td>
</tr>
</tbody>
</table>
</div>
<p><strong>函数逼近器选择</strong>  </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>线性模型</strong></td>
<td>计算简单，理论收敛性明确</td>
<td>表达能力有限</td>
<td>低维状态空间</td>
</tr>
<tr>
<td><strong>多项式基函数</strong></td>
<td>可扩展非线性特征</td>
<td>特征工程依赖先验知识</td>
<td>中等维度状态空间</td>
</tr>
<tr>
<td><strong>神经网络</strong></td>
<td>高表达能力，自动特征提取</td>
<td>训练不稳定，易过拟合</td>
<td>高维/连续状态空间</td>
</tr>
<tr>
<td><strong>决策树</strong></td>
<td>可解释性强</td>
<td>不适合连续动作空间</td>
<td>离散状态空间</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Sarsa-with-Function-Approximation"><a href="#Sarsa-with-Function-Approximation" class="headerlink" title="Sarsa with Function Approximation"></a>Sarsa with Function Approximation</h3><ul>
<li><p><strong>动作值函数近似</strong>：用 $Q(s,a; \mathbf{w}) \approx Q^\pi(s,a)$  </p>
</li>
<li><p><strong>TD误差</strong>：  </p>
<script type="math/tex; mode=display">
\delta_t = r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}; \mathbf{w}) - Q(s_t, a_t; \mathbf{w})</script></li>
<li><p><strong>参数更新</strong>：  </p>
<script type="math/tex; mode=display">
\mathbf{w} \leftarrow \mathbf{w} + \alpha \delta_t \nabla_{\mathbf{w}} Q(s_t, a_t; \mathbf{w})</script></li>
</ul>
<h3 id="Q-learning-with-Function-Approximation"><a href="#Q-learning-with-Function-Approximation" class="headerlink" title="Q-learning with Function Approximation"></a>Q-learning with Function Approximation</h3><ul>
<li><p><strong>TD误差</strong>：  </p>
<script type="math/tex; mode=display">
\delta_t = r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a'; \mathbf{w}) - Q(s_t, a_t; \mathbf{w})</script></li>
<li><p><strong>参数更新</strong>：  </p>
<script type="math/tex; mode=display">
\mathbf{w} \leftarrow \mathbf{w} + \alpha \delta_t \nabla_{\mathbf{w}} Q(s_t, a_t; \mathbf{w})</script></li>
</ul>
<h3 id="Deep-Q-Learning-DQN"><a href="#Deep-Q-Learning-DQN" class="headerlink" title="Deep Q-Learning (DQN)"></a>Deep Q-Learning (DQN)</h3><ul>
<li><p><strong>神经网络逼近</strong>：用深度网络 $Q(s,a; \mathbf{w})$ 替代线性模型  </p>
</li>
<li><p><strong>经验回放（Experience Replay）</strong>：<br>存储经验 $(s<em>t, a_t, r</em>{t+1}, s_{t+1})$​ 到缓冲池，随机采样打破相关性  </p>
<script type="math/tex; mode=display">\text{Sample } (s_i, a_i, r_{i+1}, s_{i+1}) \sim \mathcal{D}</script><script type="math/tex; mode=display">\text{Update } Q \text{ using TD error: } \delta = r + \gamma \max_{a'} Q(s', a'; \mathbf{w}^-) - Q(s, a; \mathbf{w})</script></li>
</ul>
<ul>
<li><strong>目标网络（Target Network）</strong>：<br>Q-learning的TD目标 $r + \gamma \max_{a’} Q(s’,a’;\mathbf{w})$ 与当前网络参数 $\mathbf{w}$ 相关，导致目标值随训练不断变化（目标移动），训练震荡。所以使用独立参数 $\mathbf{w}^-$ 计算TD目标，缓解波动，稳定训练：  <script type="math/tex; mode=display">
\delta_t = r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a'; \mathbf{w}^-) - Q(s_t, a_t; \mathbf{w})</script></li>
</ul>
<p>训练流程  </p>
<ol>
<li>初始化当前网络 $Q(\mathbf{w})$ 和目标网络 $Q(\mathbf{w}^-)$  </li>
<li>收集经验并存入缓冲池  </li>
<li>随机采样小批量经验，计算TD误差  </li>
<li>反向传播更新 $\mathbf{w}$  </li>
<li>每隔 $C$ 步同步 $\mathbf{w}^- \leftarrow \mathbf{w}$  </li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Initialize Q-network Q(w) <span class="keyword">and</span> target network Q(w^-) <span class="keyword">with</span> w^- = w</span><br><span class="line">Initialize replay buffer D</span><br><span class="line"><span class="keyword">for</span> episode = <span class="number">1</span> to M:</span><br><span class="line">    s = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t = <span class="number">1</span> to T:</span><br><span class="line">        <span class="comment"># 1. ε-贪婪策略选择动作</span></span><br><span class="line">        a = ε-greedy(Q(s; w))</span><br><span class="line">        <span class="comment"># 2. 执行动作，获取经验</span></span><br><span class="line">        s<span class="string">&#x27;, r, done = env.step(a)</span></span><br><span class="line"><span class="string">        # 3. 存储经验到缓冲池</span></span><br><span class="line"><span class="string">        D.add((s, a, r, s&#x27;</span>, done))</span><br><span class="line">        s = s<span class="string">&#x27;</span></span><br><span class="line"><span class="string">        # 4. 从缓冲池采样并更新网络</span></span><br><span class="line"><span class="string">        if len(D) &gt; batch_size:</span></span><br><span class="line"><span class="string">            batch = D.sample(batch_size)</span></span><br><span class="line"><span class="string">            # 计算目标Q值</span></span><br><span class="line"><span class="string">            targets = r + γ * max(Q(s&#x27;</span>; w^-)) * (<span class="number">1</span> - done)</span><br><span class="line">            <span class="comment"># 计算损失</span></span><br><span class="line">            loss = MSE(Q(s,a; w), targets)</span><br><span class="line">            <span class="comment"># 梯度下降更新w</span></span><br><span class="line">            w = AdamOptimizer.minimize(loss)</span><br><span class="line">            <span class="comment"># 每隔C步同步目标网络</span></span><br><span class="line">            <span class="keyword">if</span> t % C == <span class="number">0</span>:</span><br><span class="line">                w^- = w</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="策略梯度方法（Policy-Gradient-Methods）"><a href="#策略梯度方法（Policy-Gradient-Methods）" class="headerlink" title="策略梯度方法（Policy Gradient Methods）"></a>策略梯度方法（Policy Gradient Methods）</h2><p><strong>策略梯度的基本思想</strong></p>
<p><strong>核心概念</strong></p>
<ul>
<li><p><strong>直接优化策略</strong>：不同于值函数方法（如Q-learning）间接通过优化值函数来改进策略，策略梯度方法<strong>直接参数化策略</strong> $\pi_\theta(a|s)$（如神经网络），并通过梯度上升最大化预期回报。</p>
</li>
<li><p><strong>策略参数化</strong>：使用可微函数（如神经网络）表示策略，参数为 $\theta$，输出动作的概率分布：  </p>
<script type="math/tex; mode=display">
\pi_\theta(a|s) = \mathbb{P}[a|s; \theta]</script></li>
<li><p><strong>目标</strong>：找到最优参数 $\theta^*$，使得策略在环境中获得的累积回报最大。</p>
</li>
</ul>
<p> <strong>衡量策略优劣的指标</strong></p>
<p><strong>(1) 平均奖励（Average Reward）</strong></p>
<p>适用于<strong>持续任务</strong>（无终止状态）：  </p>
<script type="math/tex; mode=display">
J(\theta) = \lim_{T \to \infty} \frac{1}{T} \mathbb{E}\left[ \sum_{t=1}^T r_t \right]</script><p><strong>(2) 折扣平均奖励（Discounted Return）</strong></p>
<p>适用于<strong>有终止状态</strong>的任务：  </p>
<script type="math/tex; mode=display">
J(\theta) = \mathbb{E}\left[ \sum_{t=0}^\infty \gamma^t r_{t+1} \right]</script><p>其中轨迹的初始状态分布为 $s_0 \sim \mu(s_0)$（如均匀分布）。</p>
<p><strong>目标函数的梯度计算</strong></p>
<p><strong>策略梯度定理（Policy Gradient Theorem）</strong></p>
<p>无论采用平均奖励还是折扣奖励目标，策略梯度均可表示为：</p>
<script type="math/tex; mode=display">
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \ln \pi_\theta(a|s) \cdot Q^{\pi_\theta}(s,a) \right]</script><ul>
<li><p><strong>直观解释</strong>：通过增加高回报动作的概率来优化策略。</p>
</li>
<li><p><strong>推导关键</strong>：使用<strong>似然比技巧（Likelihood Ratio Trick）</strong>将梯度转换为期望形式，从而可通过样本近似。</p>
</li>
<li><p><strong>$\nabla<em>\theta \ln \pi</em>\theta(a|s)$</strong>：策略的概率变化方向。</p>
</li>
<li><p><strong>$Q^{\pi_\theta}(s,a)$</strong>：动作的价值，作为权重调整更新幅度。</p>
</li>
</ul>
<p><strong>两种场景的梯度形式</strong></p>
<ol>
<li><p><strong>平均奖励</strong>：  </p>
<script type="math/tex; mode=display">
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \ln \pi_\theta(a|s) \cdot \left( Q^{\pi_\theta}(s,a) - b(s) \right) \right]</script><p>（$b(s)$ 为基线函数，用于降低方差）</p>
</li>
<li><p><strong>折扣奖励</strong>：  </p>
<script type="math/tex; mode=display">
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \ln \pi_\theta(a|s) \cdot G_t \right]</script><p>（$G<em>t = \sum</em>{k=0}^\infty \gamma^k r_{t+k+1}$ 为累积回报）</p>
</li>
</ol>
<p><strong>梯度上升算法（REINFORCE）</strong></p>
<p><strong>算法原理</strong></p>
<p>REINFORCE 是最基础的策略梯度算法，属于<strong>蒙特卡洛方法</strong>（需完整轨迹）。其核心是通过采样轨迹估计梯度，更新策略参数。</p>
<p><strong>算法步骤</strong></p>
<ol>
<li><p><strong>初始化策略参数</strong> $\theta$。</p>
</li>
<li><p><strong>循环以下步骤</strong>：<br>a. <strong>生成轨迹</strong>：使用当前策略 $\pi<em>\theta$ 与环境交互，得到轨迹 $\tau = (s_0,a_0,r_1,s_1,a_1,r_2,\dots,s_T)$。<br>b. <strong>计算累积回报</strong>：对每个时间步 $t$，计算 $G_t = \sum</em>{k=t}^{T} \gamma^{k-t} r_{k+1}$。<br>c. <strong>估计梯度</strong>：  </p>
<script type="math/tex; mode=display">
   \nabla_\theta J(\theta) \approx \frac{1}{T} \sum_{t=0}^{T} \nabla_\theta \ln \pi_\theta(a_t|s_t) \cdot G_t</script><p>d. <strong>更新参数</strong>：  </p>
<script type="math/tex; mode=display">
   \theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)</script><p>   （$\alpha$ 为学习率）</p>
</li>
</ol>
<p><strong>改进：引入基线（Baseline）</strong></p>
<p>为减少方差，可在梯度估计中减去基线函数 $b(s)$（通常选择状态值函数 $V(s)$）：  </p>
<script type="math/tex; mode=display">
\nabla_\theta J(\theta) \approx \frac{1}{T} \sum_{t=0}^{T} \nabla_\theta \ln \pi_\theta(a_t|s_t) \cdot (G_t - b(s_t))</script><ul>
<li><strong>常见选择</strong>：  <ul>
<li>$b(s) = V^{\pi_\theta}(s)$ → 得到 <strong>Advantage Function</strong> $A(s,a) = Q(s,a) - V(s)$  </li>
<li>通过神经网络近似 $V(s)$（如Actor-Critic方法）</li>
</ul>
</li>
</ul>
<p><strong>REINFORCE 伪代码示例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">REINFORCE</span>(<span class="params">env, policy, alpha=<span class="number">0.01</span>, gamma=<span class="number">0.99</span>, num_episodes=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">        states, actions, rewards = [], [], []</span><br><span class="line">        s = env.reset()</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">            a = policy.sample_action(s)  <span class="comment"># 按策略采样动作</span></span><br><span class="line">            s_next, r, done = env.step(a)</span><br><span class="line">            states.append(s)</span><br><span class="line">            actions.append(a)</span><br><span class="line">            rewards.append(r)</span><br><span class="line">            s = s_next</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算累积回报</span></span><br><span class="line">        G = <span class="number">0</span></span><br><span class="line">        returns = []</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">reversed</span>(rewards):</span><br><span class="line">            G = r + gamma * G</span><br><span class="line">            returns.insert(<span class="number">0</span>, G)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新策略参数</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(states)):</span><br><span class="line">            s_t, a_t, G_t = states[t], actions[t], returns[t]</span><br><span class="line">            grad_log_prob = policy.grad_log_prob(s_t, a_t)  <span class="comment"># 计算梯度</span></span><br><span class="line">            policy.theta += alpha * grad_log_prob * G_t  <span class="comment"># 梯度上升</span></span><br><span class="line">    <span class="keyword">return</span> policy</span><br></pre></td></tr></table></figure>
<p><strong>REINFORCE的优缺点</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>简单易实现</td>
<td>高方差（需大量轨迹）</td>
</tr>
<tr>
<td>支持连续动作空间</td>
<td>蒙特卡洛更新效率低</td>
</tr>
<tr>
<td>直接优化策略</td>
<td>无偏但收敛慢</td>
</tr>
</tbody>
</table>
</div>
<p><strong>策略梯度 vs 值函数方法</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>策略梯度</th>
<th>值函数方法（如Q-learning）</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>优化目标</strong></td>
<td>直接优化策略参数 $\theta$</td>
<td>优化值函数，间接推导策略</td>
</tr>
<tr>
<td><strong>动作空间</strong></td>
<td>天然支持连续动作</td>
<td>需离散化动作</td>
</tr>
<tr>
<td><strong>策略类型</strong></td>
<td>显式策略（可随机）</td>
<td>隐式策略（通常确定性）</td>
</tr>
<tr>
<td><strong>方差与偏差</strong></td>
<td>无偏，高方差</td>
<td>有偏（函数近似误差），低方差</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="Actor-Critic-方法"><a href="#Actor-Critic-方法" class="headerlink" title="Actor-Critic 方法"></a>Actor-Critic 方法</h2><p><strong>基本思想</strong></p>
<ul>
<li><strong>Actor</strong>：参数化策略 $\pi_\theta(a|s)$，负责生成动作。  </li>
<li><strong>Critic</strong>：参数化动作值函数 $Q_w(s,a)$，评估动作的优劣并提供反馈。  </li>
<li><strong>更新规则</strong>：结合策略梯度和Q值估计，直接利用Critic的反馈调整Actor。</li>
</ul>
<p><strong>算法步骤</strong></p>
<ol>
<li><strong>初始化</strong>：策略参数 $\theta$ 和Critic参数 $w$。  </li>
<li><strong>交互采样</strong>：使用当前策略生成轨迹 $(s<em>t, a_t, r</em>{t+1}, s_{t+1})$。  </li>
<li><strong>Critic更新</strong>：通过TD误差更新 $Q_w$：  <script type="math/tex; mode=display">\delta_t = r_{t+1} + \gamma Q_w(s_{t+1}, a_{t+1}) - Q_w(s_t, a_t)</script><script type="math/tex; mode=display">w \leftarrow w + \alpha_w \delta_t \nabla_w Q_w(s_t, a_t)</script></li>
<li><strong>Actor更新</strong>：利用Critic的Q值计算策略梯度：  <script type="math/tex; mode=display">\nabla_\theta J(\theta) = \mathbb{E} \left[ \nabla_\theta \ln \pi_\theta(a_t|s_t) \cdot Q_w(s_t, a_t) \right]</script><script type="math/tex; mode=display">\theta \leftarrow \theta + \alpha_\theta \nabla_\theta J(\theta)</script></li>
</ol>
<p><strong>特性</strong></p>
<ul>
<li><strong>在线更新</strong>：单步TD误差，无需等待完整轨迹。  </li>
<li><strong>高偏差低方差</strong>：Critic的Q值估计引入偏差，但方差低于蒙特卡洛方法。  </li>
</ul>
<h3 id="Advantage-Actor-Critic"><a href="#Advantage-Actor-Critic" class="headerlink" title="Advantage Actor-Critic"></a>Advantage Actor-Critic</h3><p><strong>基线不变性（Baseline Invariance）</strong></p>
<ul>
<li><strong>核心思想</strong>：在策略梯度中引入基线函数 $b(s)$，其选择不影响梯度估计的无偏性，但可降低方差。  </li>
<li><strong>数学表达</strong>：  <script type="math/tex; mode=display">\nabla_\theta J(\theta) = \mathbb{E} \left[ \nabla_\theta \ln \pi_\theta(a|s) \cdot (Q^\pi(s,a) - b(s)) \right]</script>若 $b(s)$ 与动作 $a$ 无关，则梯度估计仍无偏。  </li>
<li><strong>最优基线</strong>：选择 $b(s) = V^\pi(s)$ 时方差最小。  </li>
</ul>
<p><strong>优势函数（Advantage Function）</strong></p>
<ul>
<li><strong>定义</strong>：  <script type="math/tex; mode=display">A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)</script>表示动作 $a$ 相对于平均水平的优势。  </li>
<li><strong>特性</strong>：  <ul>
<li>$\mathbb{E}_{a \sim \pi}[A^\pi(s,a)] = 0$  </li>
<li>减少方差，提升学习稳定性。  </li>
</ul>
</li>
</ul>
<p><strong>A2C算法步骤</strong></p>
<ol>
<li><strong>定义Critic</strong>：参数化状态值函数 $V_v(s)$。  </li>
<li><strong>计算优势估计</strong>：  <script type="math/tex; mode=display">\delta_t = r_{t+1} + \gamma V_v(s_{t+1}) - V_v(s_t) \quad (\text{TD误差即优势估计})</script></li>
<li><strong>Critic更新</strong>：  <script type="math/tex; mode=display">v \leftarrow v + \alpha_v \delta_t \nabla_v V_v(s_t)</script></li>
<li><strong>Actor更新</strong>：  <script type="math/tex; mode=display">\theta \leftarrow \theta + \alpha_\theta \nabla_\theta \ln \pi_\theta(a_t|s_t) \cdot \delta_t</script></li>
</ol>
<p><strong>特性</strong></p>
<ul>
<li><strong>低方差</strong>：优势函数替代Q值，减少估计波动。  </li>
<li><strong>单Critic网络</strong>：只需估计 $V(s)$，计算量低于QAC。  </li>
</ul>
<hr>
<h3 id="Off-Policy-Actor-Critic"><a href="#Off-Policy-Actor-Critic" class="headerlink" title="Off-Policy Actor-Critic"></a>Off-Policy Actor-Critic</h3><p><strong>重要性采样（Importance Sampling）</strong></p>
<ul>
<li><strong>目标</strong>：从行为策略 $\beta(a|s)$ 生成的样本中学习目标策略 $\pi_\theta(a|s)$。  </li>
<li><strong>重要性权重</strong>：  <script type="math/tex; mode=display">\rho_t = \frac{\pi_\theta(a_t|s_t)}{\beta(a_t|s_t)}</script>修正动作概率分布的偏差。  </li>
</ul>
<p><strong>离轨策略梯度定理</strong></p>
<script type="math/tex; mode=display">
\nabla_\theta J(\theta) = \mathbb{E}_{s \sim \beta} \left[ \sum_a \pi_\theta(a|s) \nabla_\theta \ln \pi_\theta(a|s) \cdot Q^\pi(s,a) \right]</script><p>通过重要性采样转换为：  </p>
<script type="math/tex; mode=display">
\nabla_\theta J(\theta) \approx \mathbb{E}_{(s,a) \sim \beta} \left[ \rho_t \nabla_\theta \ln \pi_\theta(a|s) \cdot Q_w(s,a) \right]</script><p><strong>离轨Actor-Critic算法步骤</strong></p>
<ol>
<li><strong>生成轨迹</strong>：使用行为策略 $\beta$ 采样 $(s<em>t, a_t, r</em>{t+1}, s_{t+1})$。  </li>
<li><strong>计算重要性权重</strong>：  <script type="math/tex; mode=display">\rho_t = \frac{\pi_\theta(a_t|s_t)}{\beta(a_t|s_t)}</script></li>
<li><strong>Critic更新</strong>：通过离轨TD误差更新 $Q_w$ 或 $V_v$。  </li>
<li><strong>Actor更新</strong>：  <script type="math/tex; mode=display">\theta \leftarrow \theta + \alpha_\theta \rho_t \nabla_\theta \ln \pi_\theta(a_t|s_t) \cdot \delta_t</script></li>
</ol>
<h3 id="Deterministic-Policy-Gradient"><a href="#Deterministic-Policy-Gradient" class="headerlink" title="Deterministic Policy Gradient"></a>Deterministic Policy Gradient</h3><p><strong>确定性策略梯度定理</strong></p>
<ul>
<li><p><strong>策略定义</strong>：确定性策略 $a = \mu_\theta(s)$。  </p>
</li>
<li><p><strong>梯度公式</strong>：  </p>
<script type="math/tex; mode=display">
\nabla_\theta J(\theta) = \mathbb{E}_{s \sim \rho^\mu} \left[ \nabla_\theta \mu_\theta(s) \cdot \nabla_a Q^\mu(s,a) \big|_{a=\mu_\theta(s)} \right]</script><p>其中 $\rho^\mu(s)$ 是状态分布。  </p>
</li>
</ul>
<p><strong>确定性Actor-Critic算法（DPG）</strong></p>
<ol>
<li><strong>Critic更新</strong>：最小化Q值估计的均方误差：  <script type="math/tex; mode=display">L(w) = \mathbb{E} \left[ (r + \gamma Q_w(s', \mu_\theta(s')) - Q_w(s,a))^2 \right]</script></li>
<li><strong>Actor更新</strong>：沿Q值梯度方向提升策略：  <script type="math/tex; mode=display">\theta \leftarrow \theta + \alpha_\theta \nabla_\theta \mu_\theta(s) \cdot \nabla_a Q_w(s,a) \big|_{a=\mu_\theta(s)}</script></li>
</ol>
<p><strong>深度确定性策略梯度（DDPG）</strong></p>
<ul>
<li><strong>关键技术</strong>：  <ul>
<li><strong>经验回放</strong>：存储转移样本 $(s,a,r,s’)$。  </li>
<li><strong>目标网络</strong>：软更新Actor和Critic目标网络（$\tau \ll 1$）：  <script type="math/tex; mode=display">w^- \leftarrow \tau w + (1-\tau) w^-</script><script type="math/tex; mode=display">\theta^- \leftarrow \tau \theta + (1-\tau) \theta^-</script></li>
</ul>
</li>
</ul>
<p><strong>总结</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>策略类型</th>
<th>更新方式</th>
<th>核心优势</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>QAC</strong></td>
<td>随机策略</td>
<td>在线</td>
<td>简单直观</td>
<td>离散动作空间</td>
</tr>
<tr>
<td><strong>A2C</strong></td>
<td>随机策略</td>
<td>在线/同步</td>
<td>低方差，稳定</td>
<td>并行环境（如A3C）</td>
</tr>
<tr>
<td><strong>Off-Policy AC</strong></td>
<td>随机策略</td>
<td>离轨</td>
<td>数据高效，复用历史数据</td>
<td>机器人控制</td>
</tr>
<tr>
<td><strong>DPG/DDPG</strong></td>
<td>确定性策略</td>
<td>离轨+目标网络</td>
<td>连续动作空间高效</td>
<td>物理仿真（如MuJoCo）</td>
</tr>
</tbody>
</table>
</div>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://ember0520.github.io">春弦</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://ember0520.github.io/2025/03/19/RL_studynote/">https://ember0520.github.io/2025/03/19/RL_studynote/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></div><div class="post_share"><div class="social-share" data-image="https://picss.sunbangyan.cn/2024/01/20/0712f90713f6c7624e847c3c6f80b006.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/01/20/hello-world/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picdm.sunbangyan.cn/2024/01/20/24319fb70574ecff96ba56b58be139a3.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Hello World</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">强化学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E5%90%8D%E8%AF%8D"><span class="toc-number">1.1.</span> <span class="toc-text">关键名词</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F"><span class="toc-number">1.2.</span> <span class="toc-text">贝尔曼公式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8A%B6%E6%80%81%E5%80%BC%E5%87%BD%E6%95%B0%EF%BC%88State-Value-Function%EF%BC%89"><span class="toc-number">1.2.1.</span> <span class="toc-text">状态值函数（State Value Function）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B%EF%BC%88Bellman-Equation%EF%BC%89"><span class="toc-number">1.2.2.</span> <span class="toc-text">贝尔曼方程（Bellman Equation）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E4%BD%9C%E5%80%BC%E5%87%BD%E6%95%B0%EF%BC%88Action-Value-Function%EF%BC%89"><span class="toc-number">1.2.3.</span> <span class="toc-text">动作值函数（Action Value Function）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F"><span class="toc-number">1.3.</span> <span class="toc-text">贝尔曼最优公式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5%E4%B8%8E%E6%9C%80%E4%BC%98%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.1.</span> <span class="toc-text">最优策略与最优值函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%BC%E8%BF%AD%E4%BB%A3%EF%BC%88Value-Iteration%EF%BC%89"><span class="toc-number">1.4.</span> <span class="toc-text">值迭代（Value Iteration）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%EF%BC%88Policy-Iteration%EF%BC%89"><span class="toc-number">1.5.</span> <span class="toc-text">策略迭代（Policy Iteration）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%88%AA%E6%96%AD%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%EF%BC%88Truncated-Policy-Iteration%EF%BC%89"><span class="toc-number">1.6.</span> <span class="toc-text">截断策略迭代（Truncated Policy Iteration）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95%EF%BC%88Monte-Carlo%EF%BC%89"><span class="toc-number">1.7.</span> <span class="toc-text">蒙特卡洛方法（Monte Carlo）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E8%BF%91%E4%BC%BC%EF%BC%88Stochastic-Approximation%EF%BC%89"><span class="toc-number">1.8.</span> <span class="toc-text">随机近似（Stochastic Approximation）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88Stochastic-Gradient-Descent-SGD%EF%BC%89"><span class="toc-number">1.9.</span> <span class="toc-text">随机梯度下降（Stochastic Gradient Descent, SGD）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0%EF%BC%88Temporal-Difference-Learning%EF%BC%89"><span class="toc-number">1.10.</span> <span class="toc-text">时序差分学习（Temporal Difference Learning）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TD-Learning-of-State-Values-TD-0"><span class="toc-number">1.10.1.</span> <span class="toc-text">TD Learning of State Values (TD(0))</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TD-Learning-of-Action-Values-Sarsa"><span class="toc-number">1.10.2.</span> <span class="toc-text">TD Learning of Action Values: Sarsa</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TD-Learning-of-Action-Values-Expected-Sarsa"><span class="toc-number">1.10.3.</span> <span class="toc-text">TD Learning of Action Values: Expected Sarsa</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TD-Learning-of-Action-Values-n-step-Sarsa"><span class="toc-number">1.10.4.</span> <span class="toc-text">TD Learning of Action Values: n-step Sarsa</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TD-Learning-of-Optimal-Action-Values-Q-learning"><span class="toc-number">1.10.5.</span> <span class="toc-text">TD Learning of Optimal Action Values: Q-learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93"><span class="toc-number">1.10.6.</span> <span class="toc-text">对比总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%EF%BC%88Value-Function-Approximation%EF%BC%89"><span class="toc-number">1.11.</span> <span class="toc-text">值函数近似（Value Function Approximation）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6%EF%BC%9A%E7%8A%B6%E6%80%81%E5%80%BC%E5%87%BD%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="toc-number">1.11.1.</span> <span class="toc-text">算法框架：状态值函数估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sarsa-with-Function-Approximation"><span class="toc-number">1.11.2.</span> <span class="toc-text">Sarsa with Function Approximation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q-learning-with-Function-Approximation"><span class="toc-number">1.11.3.</span> <span class="toc-text">Q-learning with Function Approximation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Deep-Q-Learning-DQN"><span class="toc-number">1.11.4.</span> <span class="toc-text">Deep Q-Learning (DQN)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%96%B9%E6%B3%95%EF%BC%88Policy-Gradient-Methods%EF%BC%89"><span class="toc-number">1.12.</span> <span class="toc-text">策略梯度方法（Policy Gradient Methods）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Actor-Critic-%E6%96%B9%E6%B3%95"><span class="toc-number">1.13.</span> <span class="toc-text">Actor-Critic 方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Advantage-Actor-Critic"><span class="toc-number">1.13.1.</span> <span class="toc-text">Advantage Actor-Critic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Off-Policy-Actor-Critic"><span class="toc-number">1.13.2.</span> <span class="toc-text">Off-Policy Actor-Critic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Deterministic-Policy-Gradient"><span class="toc-number">1.13.3.</span> <span class="toc-text">Deterministic Policy Gradient</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2025  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 春弦</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">生生不息</div></div><link rel="stylesheet" href="https://fastly.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="Chat"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      enableQQ: true,
      appId: 'BJkws1wLZc1fY2SqF7QSIlfn-gzGzoHsz',
      appKey: 'PrCvmgDyT814N4Zx3CaX2BfN',
      avatar: 'mp',
      placeholder: '昵称填写qq账号可以显示qq头像和昵称QwQ',
      serverURLs: '',
      background: 'url(https://img.gejiba.com/images/9ac2024f841bcf49231fe2548f707e7b.gif)',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><div class="aplayer no-destroy" data-id="8654907383" data-server="tencent" data-type="playlist"data-theme="#99CCFF" data-order="list" data-fixed="true" data-preload="auto" data-autoplay="false" data-mutex="true" ></div><script data-pjax src="//npm.elemecdn.com/pace-js@1.2.4/pace.min.js"></script><script src="//code.tidio.co/v2kkmd3mbdy7y0teoggijeykpozxmuov.js" async="async"></script><script>function onTidioChatApiReady() {
  window.tidioChatApi.hide();
  window.tidioChatApi.on("close", function() {
    window.tidioChatApi.hide();
  });
}
if (window.tidioChatApi) {
  window.tidioChatApi.on("ready", onTidioChatApiReady);
} else {
  document.addEventListener("tidioChat-ready", onTidioChatApiReady);
}

var chatBtnFn = () => {
  document.getElementById("chat_btn").addEventListener("click", function(){
    window.tidioChatApi.show();
    window.tidioChatApi.open();
  });
}
chatBtnFn()
</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var qweather_key = '696455271d9d495e83ac29f408688b0d';
  var gaud_map_key = '21647a3646bf0f1e7333d3823f91b1ef';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '117.283042,31.86119';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('article-sort-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__slideInRight');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('wowpanels');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow_init.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>